import re
import sys
import random
import math
import collections
from collections import defaultdict

#####################################################################
#######################  Language Model #############################
#####################################################################

class Ngram_Language_Model: #The language model I built in the ex1
    """
        The class implements a Markov Language Model that learns a language model from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """

    def __init__(self, n=3, chars=False):
        """
        Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False.
        Explanation:
            In this function i define a dictionary of ngram and dictionary of dictionaries of all grams from 1 to n.
            In addition in case that try to define model language with n<1 i change the n to be 3.
        """

        self.n = n
        self.model_dict = defaultdict(
            int)  # a dictionary of the form {ngram:count}, holding counts of all ngrams in the specified text.
        self.model_dict_of_dicts = None  # a dictionary of ngrams dictionaries.
        self.chars = chars
        self.dict_of_vocabulary_count = None
        # If n < 1 , Ngram model Will not be able to work, so in such a case I will change n to be the default value, 3
        if n < 1:
            self.n = 3

    def build_model(self, text):
        """
            This function build the model.

            Args:
                text (str): the text to construct the model from.

            Explanation:
                This function populate a dictionary that count all ngrams in the specified text.
                Moreover this function populate a dictionary of dictionaries. Each one of the dictionaries is counting all
                grams from 1 to n.
                In addition it makes a dictionary for count the vocabulary for each n (will used for smooth)
        """
        dict_of_dicts = {}
        dict_of_vocabulary_count = {}
        if self.chars == False:  # in case of words
            tokens = [token for token in text.split(" ") if token != ""]  # create a list of all tokens
            for i in range(1, self.n + 1):
                ngrams = zip(
                    *[tokens[i:] for i in range(i)])  # Goes over the tokens and attaches each series of n words
                ngrams = [" ".join(ngram) for ngram in ngrams]  # Makes a space between the words
                dict_of_dicts[i] = collections.Counter(
                    ngrams)  # Counts the number of times a particular ngram appears and populates a dictionary
                dict_of_vocabulary_count[i] = len(
                    dict_of_dicts[i].items())  # Counts the number of distinct words in each ngram
            for i in range(1, self.n):
                is_not_context = tokens[len(tokens) - i:]
                is_not_context = ' '.join(is_not_context)
                dict_of_dicts[i][is_not_context] -= 1
            self.model_dict_of_dicts = dict_of_dicts
            self.model_dict = defaultdict(int, dict_of_dicts[self.n])
            self.dict_of_vocabulary_count = dict_of_vocabulary_count
        else:  # in case of chars
            for i in range(1, self.n + 1):
                ngrams = ["".join(j) for j in zip(*[text[i:] for i in range(i)])]
                dict_of_dicts[i] = collections.Counter(
                    ngrams)  # Counts the number of times a particular ngram appears and populates a dictionary
                dict_of_vocabulary_count[i] = len(
                    dict_of_dicts[i].items())  # Counts the number of distinct words in each ngram
            self.model_dict_of_dicts = dict_of_dicts
            self.model_dict = defaultdict(int, dict_of_dicts[self.n])
            self.dict_of_vocabulary_count = dict_of_vocabulary_count

    def get_model_dictionary(self):
        """
        Returns the dictionary class object
        """
        return self.model_dict

    def get_model_window_size(self):
        """
        Returning the size of the context window (the n in "n-gram")
        """
        return self.n

    def generate(self, context=None, n=20):
        """
        Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context will be sampled
        from the models' contexts distribution. Generation will stop before the n'th word if the
        contexts are exhausted (if no more context in the sentence).
        If the length of the specified context exceeds (or equal to) the specified n,
        the method will return the a prefix of length n of the specified context.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

            Explanation:
            if there is no context or the context is not in length of (self.n - 1 ) the function will call to complete_
            or_create function that will create a context or complete it to n-1 length and then will continue regular.
            This function will functions that will help to generate sentence according to the case (words or chars).
            If the first context is OOV the function know to generate sentence too. the generate will stop only if there
            is no more context ךater in the sentence.
        """
        get_context = True  # in case that we get context
        generated_text = ""
        if (self.chars == False):
            first_tokens = [token for token in context.split(" ") if token != ""]
            start_context_len = len(first_tokens)
        else:  # in case of chars
            start_context_len = len(context)
        if start_context_len >= n:  # this part will return the context in length n when the context is equal or exceeds n.
            if start_context_len == n:
                return context
            else:
                return context.rsplit(' ', start_context_len - n)[0]
        # if there is no context
        elif context is None or context == "":  # create a context when not get one.
            context = ""
            context = self.create_or_complete_context(context,
                                                      get_context)  # the function know when to create or complete.
            get_context = False
        # if the context is short
        elif start_context_len < (self.n - 1):
            context = normalize_text(context)
            context = self.create_or_complete_context(context, get_context)
        # if context is in length of n-1
        else:
            context = normalize_text(context)

        generated_text = generated_text + context

        if (self.chars == False):
            tokens = [token for token in generated_text.split(" ") if token != ""]
            num_of_tokens_in_context = len(tokens)
        else:  # case of chars
            num_of_tokens_in_context = len(context)

        generated_text_len = num_of_tokens_in_context

        while generated_text_len < n:
            if self.chars == False:
                next_generate = self.generate_word(generated_text, generated_text_len, get_context)
                if next_generate == True:
                    break
                generated_text = generated_text + " " + next_generate
                generated_text_len += 1
            else:
                next_generate = self.generate_char(generated_text, generated_text_len, get_context)
                if next_generate == True:
                    break
                generated_text = generated_text + next_generate
                generated_text_len += 1

        return generated_text

    def create_or_complete_context(self, context, get_context):
        """
            This fucntion will return a context to start from.

            Args:
                context (str): the context that we get or not get.
                get_context (boolean): tell us if we get context from start or not.

            Return:
                String, Context.

            Explanation:
                There is two option for this function:
                1.to create a context when we dont get one , in this case the function
                 will sample a context of size n by distribution of ngram.
                2. complete a context when we get a short one. in this case the function will complete the context
                    until the context will be in length of n-1.

        """
        tokens = [token for token in context.split(" ") if token != ""]
        if self.chars == False:
            context_len = len(tokens)
        else:
            context_len = len(context)
        dist_dict = {}  # for complete
        current_context = context

        if context_len == 0:  # need to create context (work for chars and for words)
            if self.n > 1:
                dict = self.model_dict_of_dicts[self.n - 1]
            else:
                dict = self.model_dict_of_dicts[self.n]
            sum_val = sum(dict.values())
            for item in dict.keys():
                prob = dict[item] / sum_val
                dist_dict[item] = prob
            sample_context = random.choices(list(dict.keys()), weights=dist_dict.values(), cum_weights=None, k=1)
            return sample_context[0]

        else:
            while context_len < (self.n - 1):
                if self.chars == False:  # in case of words
                    generated_word = self.generate_word(current_context, context_len, get_context)
                    current_context = current_context + " " + generated_word
                    context_len += 1
                else:  # case of chars
                    generated_char = self.generate_char(current_context, context_len, get_context)
                    current_context = current_context + generated_char
                    context_len += 1
            return current_context

    def generate_word(self, genereted_text, context_len, get_context):
        """
            This fucntion will return the n+1 word (generate word).

            Args:
                genereted_text (str): the text that generated until now.
                context_len(int): the length of the context to generate from.
                get_context (boolean): tell us if we get context from start or not.

            Return:
                String , word to be generate.

            Explanation:
                The function get context and according to the length of the context know how to generate.
                In case of OVV (not context) we have 2 cases:
                1.we get a context in the start but the context OOV so the function still generate a word because i want
                    to generate text.
                2. the other case is a case of OOV after we start generate and in this case the generate will be stoped.

        """
        OOV_flag = False
        count_valuse = 0  # need it if OVV
        dist_dict = {}
        candidates_dict = {}
        context = genereted_text
        if self.n > 1:
            if self.n <= context_len:  # make a context in length of (n-1) (n is of the ngram)
                words_for_context = context.split()[-(self.n - 1):]
                context = ""
                for i in range(0, self.n - 1):
                    context = context + words_for_context[i] + " "
                context = context.rstrip()
            tokens = [token for token in context.split(" ") if token != ""]
            cur_context_len = len(tokens)
            candidates_dict = self.candidates(context, context_len)

        else:  # in case that self.n = 1
            OOV_flag = True
            cur_context_len = 1
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()
        # in case of OOV
        if len(candidates_dict) == 0 and get_context == True:
            OOV_flag = True
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()

        if len(candidates_dict) == 0:  # stop when no more context
            stop_generate_flag = True
            return stop_generate_flag
        # calculate the probability
        for candidate in candidates_dict.keys():
            context_count = candidates_dict[candidate]
            if OOV_flag == True:
                prob = self.model_dict_of_dicts[1][candidate] / sum(count_valuse)
                dist_dict[candidate] = prob
            else:
                dist_dict[candidate] = self.calc_prob_for_generate(candidate, context, context_count, cur_context_len)
        sample = random.choices(list(dist_dict.keys()), weights=dist_dict.values(), cum_weights=None, k=1)

        return sample[0]

    def generate_char(self, genereted_text, context_len, get_context):
        """
            This fucntion will return the n+1 char (generate word).

            Args:
                genereted_text (str): the text that generated until now.
                context_len(int): the length of the context to generate from.
                get_context (boolean): tell us if we get context from start or not.

            Return:
                String , char to be generate.

            Explanation:
                the explanation is same like in words.

        """
        OOV_flag = False
        count_valuse = 0  # need it if OVV
        dist_dict = {}
        candidates_dict = {}
        context = genereted_text
        if self.n > 1:
            if self.n <= context_len:
                context = context[context_len - (
                            self.n - 1): context_len]  # make a context in length of (n-1) (n is of the ngram)
            cur_context_len = len(context)
            candidates_dict = self.candidates(context, context_len)

        else:  # in case that self.n = 1
            OOV_flag = True
            cur_context_len = 1
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()
        # in case of OOV
        if len(candidates_dict) == 0 and get_context == True:
            OOV_flag = True
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()
        # stop when no more context
        if len(candidates_dict) == 0:
            stop_generate_flag = True
            return stop_generate_flag
        # calculate the probability
        for candidate in candidates_dict.keys():
            context_count = candidates_dict[candidate]
            if OOV_flag == True:
                prob = self.model_dict_of_dicts[1][candidate] / sum(count_valuse)
                dist_dict[candidate] = prob
            else:
                dist_dict[candidate] = self.calc_prob_for_generate(candidate, context, context_count, cur_context_len)
        sample = random.choices(list(dist_dict.keys()), weights=dist_dict.values(), cum_weights=None, k=1)

        return sample[0]

    def candidates(self, context, num_of_words_in_sen):
        """
            This function will return a dictionary of optional words/chars to be generate

            Args:
                context(str) : context to check which words/chars can be candidate
                num_of_words_in_sen(int) : the length of the sentence that already generated

            Return:
                dictionary of candidates.

            Explanation:
                This function get a context and the length of the sentence until now and make a dictionary of all words
                that can be generate from this context (or empty dictionary for OOV context) and the number of time that
                the context appear ( will help later to calculate the probability).

        """
        check_in = {}
        cand_dict = {}
        context_count = 0
        tokens = [token for token in context.split(" ") if token != ""]
        if self.chars == False:
            context_len = len(tokens)
        else:
            context_len = len(context)
        if context_len == (self.n - 1):
            check_in = self.model_dict
        else:
            check_in = self.model_dict_of_dicts[context_len + 1]
        if self.chars == False:
            for item in check_in.keys():
                if self.n > num_of_words_in_sen:
                    cand_help = item.rsplit(' ', self.n - num_of_words_in_sen)
                    if cand_help[0] == context:
                        context_count = context_count + check_in[item]
                        cand_dict[cand_help[1]] = 0
                else:
                    cand_help = item.rsplit(' ', 1)
                    if cand_help[0] == context:
                        context_count = context_count + self.model_dict[item]
                        cand_dict[cand_help[1]] = 0
            for cand in cand_dict.keys():
                cand_dict[cand] = context_count
        else:
            for item in check_in.keys():
                cand_help = item[(len(item) - (self.n)):len(item) - 1]
                if cand_help == context:
                    context_count = context_count + check_in[item]
                    cand_dict[item[-1]] = 0
            for cand in cand_dict.keys():
                cand_dict[cand] = context_count

        return cand_dict

    def calc_prob_for_generate(self, word, context, context_count, context_len):
        """
            This function will return the probability of a word/char to be after a context.

            Args:
                word (str): word or char to calculate probability for.
                context(str) : context that help to calculate probability.
                context_count(int) : the number of times that the context was appeared before any words/chars
                context_len(int) : lenght of the context

            Return:
                    Float - probability
            Explanation:
                calculate probability for specific word or char to be after a context.

        """

        "Probability of `word` to be after context."
        if self.chars == False:
            word_with_context = context + " " + word
        else:
            word_with_context = context + word
        count_of_word_after_context = self.model_dict_of_dicts[context_len + 1][word_with_context]
        return count_of_word_after_context / context_count

    def evaluate(self, text):
        """
           Returns the log-likelihood of the specified text to be a product of the model.
           Laplace smoothing will be applied if necessary.

           The function will normalize the text and create ngrams dictionary , then will check if need to calculate
           probability by using smooth or not and return the log-likelihood prob.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.

            Explanation:
                The function will create ngrams from the text and in addition will create ngrams from 1 to (n-1)gram
                to calculate the probabilities for the starts words too.
                This function use calculate_prob_for_evaluate or smooth according to the situation to calculate the
                total probability for text to be product from the model.
        """

        log_prob = 0
        smooth_flag = False  # tell us if we need to calculate by using laplace smooth or not.
        if self.chars == False:
            norm_text = normalize_text(text)
            tokens = [token for token in norm_text.split(" ") if token != ""]  # create a list of all tokens
            if len(tokens) >= self.n:
                ngrams_from_text = zip(*[tokens[i:] for i in range(self.n)])
            else:
                ngrams_from_text = zip(*[tokens[i:] for i in range(len(tokens))])  # for evaluate when text < n
            ngrams_from_text = [" ".join(ngram) for ngram in ngrams_from_text]  # Makes a space between the words


            # spacial case, when n = 1
            if self.n == 1:
                prob = (self.model_dict_of_dicts[1][norm_text]) / (
                    sum(self.model_dict_of_dicts[1].values()))
                if prob == 0:
                    return math.log(1 / (sum(self.model_dict_of_dicts[1].values())))  # smooth
                return math.log(prob)
            # we want to calc the probability to generate the start of the sentence before the ngram, this code
            # add the start of the sentence until the ngram to the list and calculate the prob too.
            if self.n - 1 <= len(tokens):
                ran = self.n
            else:
                ran = len(tokens)
                # add the words until (n-1)gram to the dict to calculate the probability of them too.
            temp_list = []
            for j in range(1, ran):
                ngram_minus_1_dict = zip(*[tokens[i:] for i in range(j)])
                ngram_minus_1_dict = [" ".join(ngram) for ngram in ngram_minus_1_dict]
                temp_list.append(ngram_minus_1_dict[0])
            ngrams_from_text += temp_list

        else:
            nt = normalize_text(text, True)
            ngrams_from_text = [nt[i:i + self.n] for i in range(len(nt) - self.n + 1)]

            temp_list = []
            for j in range(1, self.n):
                ngram_minus_1_dict = [text[i:i + j] for i in range(len(text) - j + 1)]
                temp_list.append(ngram_minus_1_dict[0])
            ngrams_from_text += temp_list
        for ngram in ngrams_from_text:
            if (self.chars == False):
                tokens = [token for token in ngram.split(" ") if token != ""]
                ngram_size = len(tokens)
            else:
                ngram_size = len(ngram)
            count = self.model_dict_of_dicts[ngram_size][ngram]  # check if the ngram is in the model_dict
            if count == 0:
                smooth_flag = True
        # if a laplace smooth is True:
        if smooth_flag:
            for item in ngrams_from_text:
                log_w = self.smooth(item)  # calculates the likelihood prob , using smooth
                log_prob += log_w
        else:
            for item in ngrams_from_text:
                log_w = self.calculate_prob_for_evaluate(item)  # calculates the likelihood prob
                log_prob += log_w

        return log_prob

    def calculate_prob_for_evaluate(self, ngram):
        """
            Returns the (log) probability of the specified ngram.

            Args:
                ngram (str): the ngram to calculate probability for.

            Returns:
                Float , probability.

            Explanations:
                For each case (words or chars) this function will calculate the (log) probability of the specified ngram.

        """

        if (self.chars == False):  # when the case is words
            tokens = [token for token in ngram.split(" ") if token != ""]  # split ngram to tokens
            ngram_size = len(tokens)  # count how many word we have in the ngram
            if ngram_size < self.n:  # for calculate the prob for the start of the sentence (when its not ngram)
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    prob = (dict[ngram]) / (
                        sum(self.model_dict_of_dicts[ngram_size + 1].values()))  # calculate probability
                    return math.log(prob)
                else:
                    n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    c_i = dict[ngram]
                    prob = (c_i) / (dict_minus_1[n_minus_1_gram])

                    return math.log(prob)
            # when the ngram size equal to n or bigger
            else:
                n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
            c_i = self.model_dict[ngram]

            prob = (c_i) / (dict_minus_1[n_minus_1_gram])


            return math.log(prob)

        else:  # in case of chars
            ngram_size = len(ngram)  # count how many chars we have in the ngram
            if ngram_size < self.n:  # for calculate the prob for the start of the sentence (when its not ngram)
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    prob = (dict[ngram]) / (
                        sum(self.model_dict_of_dicts[ngram_size + 1].values()))  # calculate probability
                    return math.log(prob)
                else:
                    char_minus_1_gram = ngram[0:len(ngram) - 1]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    c_i = dict[ngram]
                    prob = (c_i) / (dict_minus_1[char_minus_1_gram])
                    return math.log(prob)

            # when the ngram size equal to n or bigger
            else:
                char_minus_1_gram = ngram[0:len(ngram) - 1]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]

            c_i = self.model_dict[ngram]
            # else, if not out of vocabulary:
            prob = c_i / dict_minus_1[char_minus_1_gram]
            return math.log(prob)

    def smooth(self, ngram):
        """
            Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.

            Explanation:
                this function calculate the probability the same like the calculate_prob_for_evaluate, but add 1 to the
                numerator and |v| for denominator according to laplace theory.
        """

        if (self.chars == False):  # when the case is words
            tokens = [token for token in ngram.split(" ") if token != ""]  # split ngram to tokens
            ngram_size = len(tokens)  # count how many word we have in the ngram
            if ngram_size < self.n:
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    v = self.dict_of_vocabulary_count[ngram_size]
                    prob = (dict[ngram] + 1) / (
                                v + sum(self.model_dict_of_dicts[ngram_size + 1].values()))  # calculate probability
                    return math.log(prob)

                else:
                    n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    v = len(dict.items())
                    c_i = dict[ngram]
                    prob = (c_i + 1) / (v + dict_minus_1[n_minus_1_gram])
                    return math.log(prob)

            else:
                n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
            v = self.dict_of_vocabulary_count[self.n]
            c_i = self.model_dict[ngram]
            prob = (c_i + 1) / (dict_minus_1[n_minus_1_gram] + v)
            return math.log(prob)

        else:  # in case of chars
            ngram_size = len(ngram)  # count how many chars we have in the ngram
            if ngram_size < self.n:  # for calculate the prob for the start of the sentence (when its not ngram)
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    v = self.dict_of_vocabulary_count[ngram_size]
                    prob = (dict[ngram] + 1) / (
                                v + sum(self.model_dict_of_dicts[ngram_size + 1].values()))  # calculate probability
                    return math.log(prob)
                else:
                    char_minus_1_gram = ngram[0:len(ngram) - 1]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    v = len(dict.items())
                    c_i = dict[ngram]
                    prob = (c_i + 1) / (v + dict_minus_1[char_minus_1_gram])
                    return math.log(prob)

            # when the ngram size equal to n or bigger
            else:
                char_minus_1_gram = ngram[0:len(ngram) - 1]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]

            v = self.dict_of_vocabulary_count[self.n]
            c_i = self.model_dict[ngram]
            prob = (c_i + 1) / (dict_minus_1[char_minus_1_gram] + v)
            return math.log(prob)


def normalize_text(text, chars_flag=False):
    """
       The function Returns a normalized version of the specified string.

      Args:
        text (str): the text to normalize
        chars_flag (boolean , defaults to False) : this flag help to know if we need to do a normalize to words or chars
                                                    i need to get it because this function is not in the class of N-gram.
      Returns:
        string. the normalized text.

      Explanation:
      replacement - replacement of special characters and spaces between lines in both cases (words and chars) with space
                    or in special case with "." when its an end of sentence. its help me to get sentences that is not
                    containing signs unrelated to the sentence.

      re.sub - Checks for each sign whether there is space before or after it. If not, add a space to make a separation.
    """
    # replacement of special characters and spaces between lines in both cases (words and chars)
    text = text.replace('\n', " ")
    text = text.replace('^', " ")
    text = text.replace('*', " ")
    text = text.replace('...', " ")
    text = text.replace('..', " ")
    text = text.replace('#', " ")
    text = text.replace('❤', " ")
    text = text.replace('\\', " ")
    text = text.replace('@', " ")
    text = text.replace("-", " ")
    text = text.replace('_', " ")
    text = text.replace('"', " ")
    text = text.replace('(', " ")
    text = text.replace(')', " ")
    text = text.replace(';', " ")
    text = text.replace('<s>', ".")

    if chars_flag == False:  # case of words
        lower_text = text.lower()  # Turns all text into small letters
        norm_text = re.sub('(?<! )(?=[.,!&*?/<>:+-])|(?<=[".,!&*?/:<>+-])(?! )', r' ', lower_text)
        return norm_text
    else:  # case of chars
        text = ' '.join(text.split())
        norm_text = text.lower()
        return norm_text



#####################################################################
#######################  Spell Checker #############################
#####################################################################

class Spell_Checker:
    """The class implements a context sensitive spell checker. The corrections
        are done in the Noisy Channel framework, based on a language model and
        an error distribution model.
    """

    def __init__(self , lm=None):
        """Initializing a spell checker object with a language model as an
        instance  variable. The language model support the evaluate()
        and the get_model() functions as defined in assignment #1.

        Args:
            lm: a language model object. Defaults to None.
        """
        self.language_model = lm


    def build_model(self, text, n=3):
        """Returns a language model object built on the specified text. The language
            model should support evaluate() and the get_model() functions as defined
            in assignment #1.

            Args:
                text (str): the text to construct the model from.
                n (int): the order of the n-gram model (defaults to 3).

            Returns:
                A language model object

        """

        ngram = n
        norm_text = normalize_text(text)
        language_model = Ngram_Language_Model(ngram)
        language_model.build_model(norm_text)
        self.language_model = language_model
        self.n = ngram

        temp_unigram_dict = {}
        ngram_dict = language_model.get_model_dictionary()  # get the ngram dictionary from the language model
        for k in ngram_dict.keys():  # goes through all the keys in the dictionary.
            word = k.split(' ', 1)[0]  # take each time the first word from the ngram (like that i goes through all the words)
            if word in temp_unigram_dict.keys():
                temp_unigram_dict[word] += ngram_dict[k]  # add the number of times that the word appears if the word already in the dict
            else:
                temp_unigram_dict[word] = ngram_dict[k]  # add the number of times that the word appears
        self.unigram_dict = temp_unigram_dict

        self.one_letter_dict = collections.Counter(list(norm_text))
        list_of_two_letters = []
        for i in range(len(norm_text)):
            list_of_two_letters.append(norm_text[i:i + 2])
        self.two_letters_dict = collections.Counter(list_of_two_letters)

        return language_model

    def add_language_model(self, lm):
        """Adds the specified language model as an instance variable.
        (Replaces an older LM dictionary if set)

        Args:
            lm: a language model object

        Explanation:
            This function get the ngram dictionary from language model and  make a dictionary of all the words in the
            corpus and the number of times they appear in it.
            In addition the function get the n of the ngram.
         """
        temp_unigram_dict = {}
        self.language_model = lm
        #make a unigram dictionary
        ngram_dict = lm.get_model_dictionary() #get the ngram dictionary from the language model
        for k in ngram_dict.keys(): #goes through all the keys in the dictionary.
            word = k.split(' ',1)[0] #take each time the first word from the ngram (like that i goes through all the words)
            if word in temp_unigram_dict.keys():
                temp_unigram_dict[word] += ngram_dict[k] #add the number of times that the word appears if the word already in the dict
            else:
                temp_unigram_dict[word] = ngram_dict[k] # add the number of times that the word appears
        #get the n of ngram model
        tokens = [token for token in list(ngram_dict.keys())[0].split(" ") if token != ""]
        n = len(tokens)
        self.unigram_dict = temp_unigram_dict
        self.n = n
        self.one_letter_dict , self.two_letters_dict = self.make_one_and_pair_letters_dict(self.unigram_dict)


    def make_one_and_pair_letters_dict(self , dict):
        """
        Returns dicts of one letter and two letters with count.

        Args:
            dict(dictionary): unigram dictionary {word : number of times appear}

        Returns:
            one_letter_dict(dictionary)
            two_letters_dict(dictionary)

        Explanation:
            This function produce 2 dictionaries , one for one letter and one for two letters with their counts.

        """

        one_letter_dict = {}
        pair_letters_dict = {}
        count = 0
        text = ''
        for key in dict.keys(): #make text from the unigram dict
            text += ' ' + key

        list_of_two_letters = []
        for i in range(len(text)):
            list_of_two_letters.append(text[i:i + 2])
        one_letter_list = set(text)
        pair_letters_list = set((list_of_two_letters))

        for letter in one_letter_list:
            for word in dict:
                if letter in word:
                    count += dict[word]
            one_letter_dict[letter] = count
            count = 0

        for pair in pair_letters_list:
            for word in dict:
                if pair in word:
                    count += dict[word]
            pair_letters_dict[pair] = count
            count = 0

        return one_letter_dict , pair_letters_dict


    def learn_error_tables(self, errors_file):
         """Returns a nested dictionary {str:dict} where str is in:
        <'deletion', 'insertion', 'transposition', 'substitution'> and the
        inner dict {str: int} represents the confusion matrix of the
        specific errors, where str is a string of two characters matching the
        row and column "indexes" in the relevant confusion matrix and the int is the
        observed count of such an error (computed from the specified errors file).
        Examples of such string are 'xy', for deletion of a 'y'
        after an 'x', insertion of a 'y' after an 'x'  and substitution
        of 'x' (incorrect) by a 'y';   and example of a transposition is 'xy' indicates the characters that are transposed.

        Notes:
            1. Ultimately, one can use only 'deletion' and 'insertion' and have
                'substitution' and 'transposition' derived. Again, we use all
                four types explicitly in order to keep things simple.
        Args:
            errors_file (str): full path to the errors file. File format, TSV:
                                <error>    <correct>

        Returns:
            A dictionary of confusion "matrices" by error type (dict).

        Explanation:
            Initially, the function takes the path of the text and produces a list of tuples (error , correct).
            Then the function the function goes through the tuples, find the error type and the pair of letters that
            cause to the error and then add it to the right place in the dictionary of errors.

        """

         errors_tuple_list = [] # list that contains tuples of errors
         errors_file_path = errors_file
         with open(errors_file_path) as errors:
             for line in errors:
                 line = line.rstrip().split('\t')
                 errors_tuple_list.append(tuple(line))

         error_tables_dict = {'deletion':{} , 'insertion':{} , 'transposition':{} , 'substitution':{}}
         for error_tuple in errors_tuple_list:
             error_type = self.find_error_type(error_tuple[0] , error_tuple[1])
             pair_letters = self.find_pair_letters(error_tuple[0] , error_tuple[1] , error_type)
             if pair_letters in error_tables_dict[error_type].keys():
                 error_tables_dict[error_type][pair_letters] += 1
             else:
                 error_tables_dict[error_type][pair_letters] = 1

         self.error_tables = error_tables_dict

         return error_tables_dict


    def find_error_type(self, word_with_error , correction):
        """
        Returns the type of error that caused the error.
           Args:
               word_with_error(str)
               correction(str): correct word

           Returns:
               The function returns the type of the error(deletion / insertion / substitution / transposition).

            Explanation:
               To find if the error is deletion or insertion the function just check the length of the correction.
               To find if the error is transposition or substitution just need to check if there is the same chars in
               the correction and the wrong word - if it is so its transposition.
        """
        if len(correction) > len(word_with_error):
            error_type = "deletion"
        elif len(correction) < len(word_with_error):
            error_type = "insertion"
        else:
            correct = sorted(list(correction))
            error_word = sorted(list(word_with_error))
            if correct == error_word: # if it will be equal its said that the letters just changed place and its transposition
                error_type = "transposition"
            else:
                error_type = "substitution"

        return error_type


    def add_error_tables(self, error_tables):
        """ Adds the speficied dictionary of error tables as an instance variable.
        (Replaces an older value dictionary if set)

        Args:
            error_tables (dict): a dictionary of error tables in the format
            returned by learn_error_tables()
       """
        self.error_tables = error_tables

    def evaluate(self, text):
        """Returns the log-likelihood of the specified text given the language
        model in use. Smoothing is applied on texts containing OOV words

       Args:
           text (str): Text to evaluate.

       Returns:
           Float. The float should reflect the (log) probability.
        """
        return self.language_model.evaluate(text)


    def spell_check(self, text, alpha):
        """ Returns the most probable fix for the specified text. Use a simple
        noisy channel model if the number of tokens in the specified text is
        smaller than the length (n) of the language model.

        Args:
            text (str): the text to spell check.
            alpha (float): the probability of keeping a lexical word as is.

        Return:
            A modified string (or a copy of the original if no corrections are made.)

        Explanation:
            Initially the function checks if there is a word in the text that does not appear in the vocabulary (OOV).
            If there is such a word, this is the word we would like to make a correction to.
            If we don't have a OOV word, we will try to make a correction for all words in the text and save the
            probability for each new sentence to be the correction. In the end the function will take the sentence
            with the high probability to be the fix sentence (if there is no candidates sentences return the text).

        """

        nt = normalize_text(text) # normalized the text
        words = [word for word in nt.split(" ") if word != ""] # make a list of words from the text
        candidates_dict , candidates_error_type_dicts = self.candidates_for_correction(words) # dictionary of all candidates for each word and error types dicts for each word

        word_oov_flag = False
        word_index = 0
        candidates_sentences_log_prob_dict = {} #dict in form {'sentence' : log probability)

        #check if there in a word in the text that not appear in the vocabulary
        for word in words:
            if word not in self.unigram_dict.keys():
                word_oov_flag = True
                word_to_correct = word

        for word in words:
            if word_oov_flag == True and word != word_to_correct:
                word_index += 1
                continue #if i know that there is word that is oov i want to get her without check all the others.
            for candidate in candidates_dict[word]:
                error_type = candidates_error_type_dicts[word][candidate]
                pair_letters = self.find_pair_letters(word, candidate, error_type)
                p_x_w = self.calc_p_of_x_given_w(word, candidate, alpha, error_type, pair_letters) #calculate p(x/w)
                new_sentence = self.build_new_sentence(words, candidate ,word_index) #build the new sentence with the candidate for correction
                sentence_log_prob = self.language_model.evaluate(new_sentence) #calculate the log probability of the whole sentence by using evaluate function form lm
                p_x_w_log = math.log(p_x_w * (1-alpha)) # p_w_x according to Mays et al. (1991)
                total_log_prob = p_x_w_log + sentence_log_prob
                candidates_sentences_log_prob_dict[new_sentence] = total_log_prob
            word_index += 1
        if (bool(candidates_sentences_log_prob_dict)) == False:
            return text
        #find the best sentence
        best_sentence = max(candidates_sentences_log_prob_dict,key=candidates_sentences_log_prob_dict.get)

        return best_sentence

    def build_new_sentence(self, words_in_sentence , candidate , index):
        """
         Returns the real sentence with the correction

         Args:
             words_in_sentence(list): list of the words in the real text that we get.
             candidate(str): correction to the word with error.
             index(int): the index of the word with error that need to replace.

         Returns:
             str, sentence with correction

        Explanation:
            The function get the real text that we get and returns a new sentence with a candidate for correction
        """
        new_sentence = ''
        sentence_length = len(words_in_sentence)
        if sentence_length < 2:
            return new_sentence + candidate

        for idx in range(sentence_length):
            if idx != index:
                new_sentence += words_in_sentence[idx] + ' '
            else:
                new_sentence += candidate + ' '
        new_sentence = new_sentence[:-1]

        return new_sentence

    def calc_p_of_x_given_w(self, word, candidate, alpha , error_type, pair_letters):
        """
        Returns p(x/w)

        Args:
            word(str): the word with the error
            candidate(str): the candidate for correction
            alpha(float): probability for word that is in the vocabulary
            error_type(str): the error type
            pair_letters(str): pair of letters that caused the correct word to be incorrect

        Returns:
            float: p(x/w) (probability)

        Explanation:
            The function returns the p(x/w).
            If the word is in the vocabulary the function return alpha.
            If not , the function will calculate denominator by using calc_denominator function and will calculate
            the nominator by using the errors_table.
        """

        p_x_w = 0
        if word == candidate:
            p_x_w = alpha
            return p_x_w
        else:
            denominator = self.calc_denominator(error_type , pair_letters)
            try:
                nominator = self.error_tables[error_type][pair_letters]
            except:
                nominator = 0
            if nominator == 0 :
                nominator = 1

        p_x_w = nominator / denominator
        return p_x_w


    def calc_denominator(self, error_type , pair_letters ):
        """
        Returns dominator.

        Args:
            error_type(str): the error type
            pair_letters(str): pair of letters that caused the correct word to be incorrect

        Returns:
            denominator(int): the denominator to calc p(x/w)

        Explanation:
            This function calculate denominator according to the error type and the pair of letters that caused to the
            error.
            The function do it by using one_letter_dict and two_letter_dict (dictionaries that have the letters
            and the number of times they appear).
            If in the end denominator is 0, the function will make smooth and the denominator will be the sum of
            vocabulary valuse.

        """
        denominator = 0
        one_letter_vocabulary = sum(self.one_letter_dict.values())
        two_letters_vocabulary = sum(self.two_letters_dict.values())

        if error_type == 'deletion':
            if pair_letters[0] == '#':
                for word in self.unigram_dict.keys():
                    if pair_letters[1] == word[0]:
                        denominator += self.unigram_dict[word]
            else:
                if pair_letters in self.two_letters_dict.keys():
                    denominator = self.two_letters_dict[pair_letters]

        elif error_type == 'insertion':
            if pair_letters[0] == '#':
                denominator = one_letter_vocabulary
            else:
                if pair_letters[0] in self.one_letter_dict.keys():
                    denominator = self.one_letter_dict[pair_letters[0]]

        elif error_type == 'substitution':
            if pair_letters[1] in self.one_letter_dict.keys():
                denominator = self.one_letter_dict[pair_letters[1]]

        else: #error_type == 'transposition'
            if pair_letters in self.two_letters_dict.keys():
                denominator = self.two_letters_dict[pair_letters]

        if denominator == 0:
            denominator = two_letters_vocabulary

        return denominator


    def find_pair_letters(self, word_with_error , candidate_for_correction , error_type ):
        """
        Returns the pair of letters that caused the correct word to be incorrect.

        Args:
            word_with_error(str): word with error.
            candidate_for_correction(str): candidate to be a correction to the word with the error.
            error_type(str): type of error (deletion / insertion / substitution / transposition)

        Returns:
            string of pair letters(str).

        Explanation:
            The function get a wrong word ,a candidate to correction and the type of the error and according to the type
            of error, the function makes comparison between the word with the error and the candidate and find the pair
            of letter.
        """
        error_word_length = len(word_with_error)
        candidate_length = len(candidate_for_correction)
        pair_letters = None

        if error_type == 'deletion': # xy typed as x
            pair_letters = word_with_error[error_word_length - 1] + candidate_for_correction[error_word_length] # i run until the len of word_with_error so i initializing the pair.
            for index in range(error_word_length): # i run until length of the word_with_error because it shorter.
                if candidate_for_correction[index] != word_with_error[index]:
                    if index != 0 :
                        pair_letters = word_with_error[index - 1] + candidate_for_correction[index]
                    else:
                          pair_letters = '#' + candidate_for_correction[index]
                    break # stop the for loop because the letters were found

        if error_type == 'insertion':
            pair_letters = candidate_for_correction[candidate_length - 1] + word_with_error[candidate_length] # i run until the len of word_with_error so i initializing the pair.
            for index in range(candidate_length): # i run until length of the candidate because it shorter.
                if candidate_for_correction[index] != word_with_error[index]:
                    if index != 0 :
                        pair_letters = candidate_for_correction[index-1] + word_with_error[index]
                    else:
                        pair_letters = '#' + word_with_error[index]
                    break # stop the for loop because the letters were found

        #in substitution and transposition not need to make initializing to pair_letters because the wrong and candidate are in the same len.
        if error_type == 'substitution':
            for index in range(candidate_length):
                if candidate_for_correction[index] != word_with_error[index]:
                    pair_letters = word_with_error[index] + candidate_for_correction[index]
                    break

        if error_type == 'transposition':
            for index in range(candidate_length - 1 ): # i run only until length-1 because the last letter do not have we who to swap.
                if candidate_for_correction[index] != word_with_error[index]:
                    pair_letters = candidate_for_correction[index] + candidate_for_correction[index + 1]
                    break

        return pair_letters


    def edits1(self, word):
        """ Returns all edits that are one edit away from `word` and from which error the correction came  .

           Args:
               word (str): word

           Returns:
              set of all words that are one edit away from `word` (I return set to avoid from duplications).
              dictionary of all candidates and from which error they came.

            Explanation:
            The function take the word, make a split and then produces possible word corrections from each one of the
            error types.
        """
        letters = 'abcdefghijklmnopqrstuvwxyz'
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        insertion = [L + R[1:] for L, R in splits if R] #delete a char
        transposition = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        substitution = [L + c + R[1:] for L, R in splits if R for c in letters]
        deletion = [L + c + R for L, R in splits for c in letters] # add char

        candidates_error_type_dict = {}

        for word in insertion:
            candidates_error_type_dict[word] = 'insertion'
        for word in transposition:
            candidates_error_type_dict[word] = 'transposition'
        for word in substitution:
            candidates_error_type_dict[word] = 'substitution'
        for word in deletion:
            candidates_error_type_dict[word] = 'deletion'

        return set(deletion + transposition + substitution + insertion) , candidates_error_type_dict


    def known(self, words , error_type_dict):
        """
        This function returns only the words that appear in the corpus (known words).

        Args:
            words(list): list of words (words that are candidates to be corrected for error).

        Returns:
            set of all words that appear in the corpus
            dictionary of all candidates and from which error they came.


        Explanation:
            The function get list of words and check if each one of them appear in the corpus. if it is, add the word
            to the set and return the set.
        """
        cand_error_type_dict = {}
        corpus_words = self.unigram_dict.keys()
        known_words = set(word for word in words if word in corpus_words)
        for word in known_words:
            cand_error_type_dict[word] = error_type_dict[word]

        return known_words , cand_error_type_dict

    def candidates_for_correction(self, words):
        """
            This function get list of words from the text that we get, make candidates for each one of the words and then
            returns dictionary with word and her candidates for correction.
        Args:
            words(list): list of words.

        Returns:
            dictionary { key = word : value = list of candidates}
            dictionary of all candidates and from which error they came { word : error type}.


        Explanation:
            The function get list of words, produce candidates that are in distance 1 for each one of the words by using
            edits1 function, then the function removes candidates who are not in the corpus.
            finally the function add the word itself as a candidates and return a dictionary of word and her candidates.
        """
        candidates_dict = {}
        dicts_of_errors_type_dict = {}
        for word in words:
            all_candidates, error_types = self.edits1(word)
            candidates_in_corpus, cand_error_types = self.known(all_candidates, error_types) #return only candidates that are in the corpus
            candidates_dict[word] = candidates_in_corpus
            dicts_of_errors_type_dict[word] = cand_error_types
        return candidates_dict , dicts_of_errors_type_dict


def who_am_i():
    """
        Returns a dictionary with my name, id and email.
        keys=['name', 'id','email']
    """
    return {'name': 'Omer Keidar', 'id': '307887984', 'email': 'omerkei@post.bgu.ac.il'}

